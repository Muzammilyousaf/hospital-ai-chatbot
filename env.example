# Ollama Configuration (100% Free & Local)
# Install Ollama from https://ollama.ai
# Then run: ollama pull llama3
OLLAMA_API_URL=http://localhost:11434
OLLAMA_MODEL=llama3

# Alternative models: mistral, llama2, codellama
# OLLAMA_MODEL=mistral

# Vector DB Configuration
VECTOR_DB_PATH=./data/vector_db

# Hospital Data Path
HOSPITAL_DATA_PATH=./data/hospital_knowledge

# Embedding Model Configuration
EMBEDDING_MODEL=all-MiniLM-L6-v2

# Server Configuration
HOST=0.0.0.0
PORT=8000

